#%% import packages
import torch
import torch.nn as nn
from torch.utils.data import TensorDataset, DataLoader
from torch.optim.lr_scheduler import *
from tqdm import tqdm
from sklearn.metrics import f1_score
import numpy as np
import argparse


# argument parser
def parser_data(DEVICE):
    from model import config, CNN
    parser = argparse.ArgumentParser(
        prog="Sentiment Analysis", description="2023 æ˜¥ THU Introduction to Artificial Intelligence PA2 : Sentiment Analysis", add_help=True, allow_abbrev = True
    )
    parser.add_argument(
        "-e",
        "--epoch",
        dest = "epoch",
        type = int,
        default = 10, 
        help = "epoch"
    )
    parser.add_argument(
        "-lr",
        "--learning_rate",
        dest = "learning_rate",
        type = float,
        default = 0.001, # 1e-3
        help ="starting learning rate"
    )
    parser.add_argument(
        "-b",
        "--batch_size",
        dest  = "batch_size",
        type = int,
        default = 100,
        help = "batch size"
    )
    
    parser.add_argument(
        "-ml",
        "--max_length",
        dest="max_length",
        type=int,
        default=50,
        help="max sentence length",
    )
    parser.add_argument(
        "-m",
        "--model",
        dest="nn_model",
        type=str,
        default="CNN",
        help="neural network model , default is CNN, you can choose RNN_LST, MLP",
    )
    parser.add_argument(
        "-o",
        "--optimizer",
        dest = "optimizer",
        type = str,
        default = "adam",
        help= "optimizer for the neural network, default is adam, you can choose sgd, adam, adagrad, adadelta, rmsprop"
    )
    parser.add_argument(
        "-c",
        "--criterion",
        dest = "criterion",
        type = str,
        default = "cross_entropy",
        help = "criterion for the neural network, default is Cross Entropy Loss function, you can choose " 
    )
    
    parser.add_argument(
        "-s",
        "--scheduler",
        dest = "scheduler",
        type = str,
        default = "steplr",
        help = "scheduler for the neural network, default is stepLR, you can choose "
        
    )
    
    args = parser.parse_args()
    epoch = args.epoch
    learning_rate = args.learning_rate
    max_length = args.max_length
    batch_size = args.batch_size
    model = args.nn_model
    optimizer = args.optimizer
    criterion = args.criterion
    scheduler = args.scheduler
    
    # set model
    if model == "CNN":
        model = CNN(config).to(DEVICE)
    elif model == "RNN_LST":
        # model = RNN_LST(config).to(DEVICE)
        print("not completed yet")
    elif model == "MLP":
        # model = MLP(config).to(DEVICE)
        print(" not completed yet")
    else :
        IOError("model not found")
        exit(1)
    # set optimizer
    if (optimizer == "sgd"):
        optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)
    elif (optimizer == "adam"):  # default
        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
    elif (optimizer == "adagrad"):
        optimizer = torch.optim.Adagrad(model.parameters(), lr=learning_rate)
    elif (optimizer == "rmsprop"):
        optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)
    elif (optimizer == "adadelta"):
        optimizer = torch.optim.Adadelta(model.parameters(), lr=learning_rate)
    elif (optimizer == "adamw"):
        optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)
    elif (optimizer == "sparseadam"):
        optimizer = torch.optim.SparseAdam(model.parameters(), lr=learning_rate)
    else :
        IOError("optimizer not found")
        exit(1)

    # set criterion
    if(criterion == "cross_entropy"): # default
        criterion = nn.CrossEntropyLoss()
    elif(criterion == "mse"):
        criterion = nn.MSELoss()
    elif(criterion == "l1l"):
        criterion = nn.L1Loss()
    elif(criterion == "bce"):
        criterion = nn.BCELoss()
    elif(criterion == "bcell"):
        criterion = nn.BCEWithLogitsLoss()
    elif(criterion == "kldiv"):
        criterion = nn.KLDivLoss()
    elif(criterion == "sl1l"):
        criterion = nn.SmoothL1Loss()
    else :
        IOError("criterion not found")
        exit(1)

    # set scheduler
    scheduler = StepLR(optimizer, step_size=5)
    
    return epoch, learning_rate, max_length, batch_size, model,optimizer, criterion, scheduler
    

#%% data-loader (build dataset)
def load_data(max_length, batch_size):
    
    from utils import vocab, s_vectors,build_dataset
    train_contents , train_labels = build_dataset("../Dataset/train.txt", vocab, max_length)
    val_contents, val_labels = build_dataset("../Dataset/validation.txt",vocab,max_length) 
    test_contents, test_labels = build_dataset("../Dataset/test.txt",vocab,max_length)
    
    # train dataset
    train_dataset = TensorDataset(
        torch.from_numpy(train_contents).type(torch.float),
        torch.from_numpy(train_labels).type(torch.long),
    )
    train_dataloader = DataLoader(
        dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=1
    )

    # validation dataset
    val_dataset = TensorDataset(
        torch.from_numpy(val_contents).type(torch.float),
        torch.from_numpy(val_labels).type(torch.long),
    )
    val_dataloader = DataLoader(
        dataset=val_dataset, batch_size=batch_size, shuffle=False, num_workers=1
    )

    # test dataset
    test_dataset = TensorDataset(
        torch.from_numpy(test_contents).type(torch.float),
        torch.from_numpy(test_labels).type(torch.long),
    )
    test_dataloader = DataLoader(
        dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=1
    )
    
    return train_dataloader, val_dataloader, test_dataloader


# train
def train(train_dataloader):
    model.train()
    
    train_loss , train_accuracy = 0.0, 0.0
    count , correct = 0,0
    full_true = []
    full_pred = []
    for _, (sentences, labels) in enumerate(train_dataloader):
        sentences = sentences.to(DEVICE)
        labels = labels.to(DEVICE)
        
        # forward
        optimizer.zero_grad()
        output = model(sentences)
        loss = criterion(output, labels)
        
        # backward
        loss.backward()
        optimizer.step()
        
        train_loss += loss.item()
        correct += (output.argmax(1) == labels).float().sum().item()
        count += len(sentences)
        full_true.extend(labels.cpu().numpy().tolist())
        full_pred.extend(output.argmax(1).cpu().numpy().tolist())
    train_loss *= batch_size
    train_loss /= len(train_dataloader.dataset)
    train_accuracy = correct / count
    
    scheduler.step()
    train_f1 = f1_score(np.array(full_true),np.array(full_pred),average = "binary")
    return train_loss, train_accuracy, train_f1
# valid and test
def valid_and_test(dataloader):
    model.eval()

    val_loss, val_acc = 0.0, 0.0
    count, correct = 0, 0
    full_true = []
    full_pred = []
    for _, (sentences, labels) in enumerate(dataloader):
        sentences, labels = sentences.to(DEVICE), labels.to(DEVICE)
        
        #forawrd
        output = model(sentences) # invokes forward()
        loss = criterion(output, labels)
        
        val_loss += loss.item()
        correct += (output.argmax(1) == labels).float().sum().item()
        count += len(sentences)
        full_true.extend(labels.cpu().numpy().tolist())
        full_pred.extend(output.argmax(1).cpu().numpy().tolist())
        
    val_loss *= batch_size
    val_loss /= len(dataloader.dataset)
    val_acc = correct / count
    f1 = f1_score(np.array(full_true), np.array(full_pred), average="binary")
    return val_loss, val_acc, f1
    
if __name__ == "__main__":
    # define DEVICE
    DEVICE = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    
    # get arguments
    EPOCHS, learning_rate, max_length, batch_size, model, optimizer,criterion,scheduler = parser_data(DEVICE)
    
    # load data
    train_dataloader, val_dataloader, test_dataloader = load_data(max_length, batch_size)
    
    # wandb.init(project=f"Valid", name=f"{model.__name__}", entity="eren-zhao")
    # wandb.config = {"learning_rate": 0.001, "epochs": 100, "batch_size": 50}
    for each in tqdm(range(1, EPOCHS + 1)):
        tr_loss, tr_acc, tr_f1 = train(train_dataloader)
        val_loss, val_acc, val_f1 = valid_and_test(val_dataloader)
        test_loss, test_acc, test_f1 = valid_and_test(test_dataloader)
        # wandb.log(
        #     {
        #         "train_loss": tr_loss,
        #         "train_acc": tr_acc,
        #          "train_f1": tr_f1,
        #         "val_loss": val_loss,
        #         "val_acc": val_acc,
        #         "val_f1": val_f1,
        #         "test_loss": test_loss,
        #         "test_acc": test_acc,
        #         "test_f1": test_f1,
        #     }
        # )
        print(
            f"for epoch {each}/{EPOCHS}, train_loss: {tr_loss:.4f}, train_acc: {tr_acc:.4f}, val_loss: {val_loss:.4f}, val_acc: {val_acc:.4f}, test_loss: {test_loss:.4f}, test_acc: {test_acc:.4f} (in average)"
        )