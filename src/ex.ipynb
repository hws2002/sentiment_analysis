{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% import packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.optim.lr_scheduler import *\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to build Dictionary...\n",
      "Build vector ended! Elapsed time: 0.17 seconds\n",
      "Starting to build vector...\n",
      "Building vector ended! Elapsed time: 2.77 seconds\n",
      "There are  9066  words that doesn't exist in Model (include duplication)\n",
      "Length of sentence vector is  53338\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import gensim\n",
    "import time\n",
    "\n",
    "#%% build Dictionary\n",
    "def build_vocab(List : list) -> dict: \n",
    "    \"\"\"\n",
    "    word to index\n",
    "    used for creating dataset\n",
    "    \"\"\"\n",
    "    print(\"Starting to build Dictionary...\"); start_time = time.time()\n",
    "    \n",
    "    Vocab = Counter()\n",
    "    for file in List: # train.txt and validation.txt\n",
    "        with open(file,\"r\",encoding=\"utf-8\") as f:\n",
    "            for line in f.readlines(): # 1\t死囚 爱 刽子手 女贼 爱 衙役 我们 爱 你们 难道 还有 别的 选择 没想到 ...\n",
    "                sentence = line.strip().split() # ['1', '死囚', '爱', '刽子手', '女贼', '爱', '衙役', ..\n",
    "                for voca in sentence[1:]: # first word is label\n",
    "                    if voca not in Vocab.keys():\n",
    "                        Vocab[voca] = len(Vocab)\n",
    "    end_time = time.time(); elapsed_time = end_time - start_time\n",
    "    print(\"Build vector ended! Elapsed time: {:.2f} seconds\".format(elapsed_time))\n",
    "    return Vocab\n",
    "\n",
    "vocab =  build_vocab([\"../Dataset/train.txt\"]) #train, test\n",
    "\n",
    "#%%build sentence vector\n",
    "n_exist = 0\n",
    "def build_vector(List : list, Vocab : dict) -> np.ndarray: \n",
    "    \"\"\"\n",
    "    word to vector (index from Vocab)\n",
    "    only used in model.py\n",
    "    \"\"\"\n",
    "    print(\"Starting to build vector...\"); start_time = time.time()\n",
    "    \n",
    "    global n_exist\n",
    "    # loaded pre-trained model\n",
    "    preModel = gensim.models.KeyedVectors.load_word2vec_format(\"../Dataset/wiki_word2vec_50.bin\",binary=True)\n",
    "    vector = np.array([np.zeros(preModel.vector_size)]*(len(Vocab)+1)) # +1 for padding (although there are already 0 row vector for words not in vocab)\n",
    "\n",
    "    for voca in Vocab: # some words don't exist in preModel -> \n",
    "        try:\n",
    "            vector[Vocab[voca]] = preModel[voca]\n",
    "        except Exception as e:\n",
    "            # TODO : Make a better way to handle this exception\n",
    "            # - pre process the data? idk...\n",
    "            n_exist += 1\n",
    "            pass\n",
    "            # print(\"An exception occurred: \" + str(e))\n",
    "            \n",
    "    end_time = time.time(); elapsed_time = end_time - start_time\n",
    "    print(\"Building vector ended! Elapsed time: {:.2f} seconds\".format(elapsed_time))\n",
    "    print(\"There are \",n_exist,\" words that doesn't exist in Model (include duplication)\")\n",
    "    print(\"Length of sentence vector is \",len(vector))# 53735~59290\n",
    "    return vector\n",
    "\n",
    "s_vectors = build_vector([\"../Dataset/train.txt\"],vocab)\n",
    "\n",
    "#%% parse data function to build dataset\n",
    "def build_dataset(path : str,vocab : dict,max_length=50): # length of single sentence to use from data\n",
    "    \"\"\"\n",
    "    returns contents and labels in numpy array\n",
    "    from train, test, validation\n",
    "    \"\"\"\n",
    "    print(\"Starting to parse data from \",path,\"...\"); start_time = time.time()\n",
    "    \n",
    "    words,labels = np.array([0]*max_length), np.array([],dtype=np.float64) # can't use integer here\n",
    "    with open(path,encoding='utf-8',errors='ignore') as f:\n",
    "        for line in f.readlines():\n",
    "            sentence = line.strip().split() # ['1', '如果', '我', '无聊',...\n",
    "            stripped_sentence = np.asarray([vocab.get(word,len(vocab)) for word in sentence [1:]])[:max_length] # strip only first max_length elements\n",
    "                                    # index vector of sentence\n",
    "                                    # if key doesn't exist in vocab, return 0 (len(Vocab) 이여야 되는거 아니냐? ㅅㅂ?)\n",
    "            # pad the content to match length\n",
    "            padding = max(max_length - len(stripped_sentence), 0)\n",
    "            stripped_sentence = np.pad(stripped_sentence, pad_width=(0, padding), mode='constant', constant_values=len(vocab))# len(Vocab)\n",
    "            \n",
    "            # append label, pos -> 1, neg ->0\n",
    "            labels = np.append(labels, int(sentence[0])) \n",
    "            \n",
    "            # append content\n",
    "            words = np.vstack([words,stripped_sentence])\n",
    "    # delete the first row of contents (to match its length with labels)\n",
    "    words = np.delete(words,0,axis=0)\n",
    "    \n",
    "    end_time = time.time();elapsed_time = end_time - start_time\n",
    "    print(\"Parsing data ended! Elapsed time: {:.2f} seconds-------------------------------\".format(elapsed_time))\n",
    "    return words, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyper params (decided in main.py)\n",
    "max_length = 50\n",
    "batch_size = 100\n",
    "learning_rate = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to parse data from  ../Dataset/train.txt ...\n",
      "Parsing data ended! Elapsed time: 5.00 seconds-------------------------------\n",
      "Starting to parse data from  ../Dataset/validation.txt ...\n",
      "Parsing data ended! Elapsed time: 0.41 seconds-------------------------------\n",
      "Starting to parse data from  ../Dataset/test.txt ...\n",
      "Parsing data ended! Elapsed time: 0.02 seconds-------------------------------\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "train_contents , train_labels = build_dataset(\"../Dataset/train.txt\",vocab, max_length)\n",
    "val_contents, val_labels = build_dataset(\"../Dataset/validation.txt\",vocab,max_length) \n",
    "test_contents, test_labels = build_dataset(\"../Dataset/test.txt\",vocab,max_length)\n",
    "\n",
    "# train dataset\n",
    "train_dataset = TensorDataset(\n",
    "    torch.from_numpy(train_contents).type(torch.float),\n",
    "    torch.from_numpy(train_labels).type(torch.long),\n",
    ")\n",
    "train_dataloader = DataLoader(\n",
    "    dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=1\n",
    ")\n",
    "\n",
    "# validation dataset\n",
    "val_dataset = TensorDataset(\n",
    "    torch.from_numpy(val_contents).type(torch.float),\n",
    "    torch.from_numpy(val_labels).type(torch.long),\n",
    ")\n",
    "val_dataloader = DataLoader(\n",
    "    dataset=val_dataset, batch_size=batch_size, shuffle=False, num_workers=1\n",
    ")\n",
    "\n",
    "# test dataset\n",
    "test_dataset = TensorDataset(\n",
    "    torch.from_numpy(test_contents).type(torch.float),\n",
    "    torch.from_numpy(test_labels).type(torch.long),\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class model_config():\n",
    "    \"\"\"\n",
    "    For all datasets we use: rectified linear units, \n",
    "    filter windows (h) of 3, 4, 5 with 100 feature maps each, \n",
    "    dropout rate (p) of 0.5, l2 constraint (s) of 3, \n",
    "    and mini-batch size of 50. \n",
    "    These values were chosen via a grid search on the SST-2 dev set.\n",
    "    ...\n",
    "    From the paper: https://arxiv.org/pdf/1408.5882.pdf\n",
    "    \"\"\"\n",
    "    update_w2v = True           # whether to update w2v \n",
    "    vocab_size = len(vocab)+1   # +1 for padding (recall that we added one more row for sentence vector)\n",
    "    n_classes = 2               # 0 -> neg, 1 -> pos | binary classification\n",
    "    embedding_dim = 50          # dimension of word embedding. same as word2vec model length 50\n",
    "    dropout_rate = 0.5          # dropout rate\n",
    "    kernel_num = 20             # number of each kind of kernel\n",
    "    kernel_sizes = [3,4,5]      # size of kernel, h (window size)\n",
    "    pretrained_embed = s_vectors# pretrained embedding matrix\n",
    "    #------------- RNN ONLY -----------------------------------------------------------------\n",
    "    hidden_size = 100           # hidden size of rnn\n",
    "    num_layers = 2              # number of layers of rnn\n",
    "\n",
    "config = model_config()\n",
    "\n",
    "#%% CNN model\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, config : model_config):\n",
    "        super(CNN,self).__init__()\n",
    "        update_w2v = config.update_w2v\n",
    "        vocab_size = config.vocab_size\n",
    "        n_class = config.n_classes\n",
    "        embedding_dim = config.embedding_dim\n",
    "        dropout_rate = config.dropout_rate\n",
    "        kernel_num = config.kernel_num\n",
    "        kernel_sizes = config.kernel_sizes\n",
    "        pretrained_embed = config.pretrained_embed\n",
    "\n",
    "        # embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size,embedding_dim)\n",
    "        self.embedding.weight.requires_grad = update_w2v\n",
    "        self.embedding.weight.data.copy_(torch.from_numpy(pretrained_embed))\n",
    "        \n",
    "        # convolution layer\n",
    "        # input channel size is 1, because we only have one channel (word embedding) \n",
    "        # kernel_size = height * width!\n",
    "        self.conv1_1 = nn.Conv2d(in_channels=1,out_channels= kernel_num,kernel_size=(kernel_sizes[0],embedding_dim),stride=1,padding = 0)\n",
    "        self.conv1_2 = nn.Conv2d(1,kernel_num,(kernel_sizes[1],embedding_dim))\n",
    "        self.conv1_3 = nn.Conv2d(1,kernel_num,(kernel_sizes[2],embedding_dim))\n",
    "        \n",
    "        # pooling layer\n",
    "        self.pool = nn.MaxPool1d\n",
    "        \n",
    "        # dropout\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        # fully connected layer\n",
    "        self.fc = nn.Linear(len(kernel_sizes)*kernel_num,n_class)\n",
    "\n",
    "    @staticmethod\n",
    "    def conv_and_pool(x,conv):\n",
    "        x = conv(x)\n",
    "        x = F.relu(x.squeeze(3)) #  concatenates 20\n",
    "        x = F.max_pool1d(x, x.size(2)).squeeze(2)\n",
    "        return x\n",
    "    \n",
    "    def forward(self,x):\n",
    "        # (batch_size,1,max_length,embedding_dim), converts sentence represented by id into batch size tensor\n",
    "        x = self.embedding(x.to(torch.int64)).unsqueeze(1)\n",
    "        x1 = self.conv_and_pool(x,self.conv1_1) # (batch_size, kernel_num)\n",
    "        x2 = self.conv_and_pool(x,self.conv1_2) # (batch_size, kernel_num)\n",
    "        x3 = self.conv_and_pool(x,self.conv1_3) # (batch_size, kernel_num)\n",
    "        # concatenate x1,x2,x3 column-wise, apply dropout, and apply fully-connected layer to get output\n",
    "        # as it's a binary classification, we use log_softmax as activation function\n",
    "        x = F.log_softmax(self.fc(self.dropout(torch.cat((x1,x2,x3),1))),dim=1)\n",
    "        return x\n",
    "    \n",
    "#%% RNN model\n",
    "\n",
    "\n",
    "\n",
    "#%% MLP model\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CNN(config).to(DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, validation and test\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = StepLR(optimizer, step_size=5)\n",
    "\n",
    "\n",
    "def train(train_dataloader):\n",
    "    model.train()\n",
    "    \n",
    "    train_loss , train_accuracy = 0.0, 0.0\n",
    "    count , correct = 0,0\n",
    "    full_true = []\n",
    "    full_pred = []\n",
    "    for _, (sentences, labels) in enumerate(train_dataloader):\n",
    "        sentences = sentences.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "        \n",
    "        # forward\n",
    "        optimizer.zero_grad()\n",
    "        output = model(sentences)\n",
    "        loss = criterion(output, labels)\n",
    "        \n",
    "        # backward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        correct += (output.argmax(1) == labels).float().sum().item()\n",
    "        count += len(sentences)\n",
    "        full_true.extend(labels.cpu().numpy().tolist())\n",
    "        full_pred.extend(output.argmax(1).cpu().numpy().tolist())\n",
    "    train_loss *= batch_size\n",
    "    train_loss /= len(train_dataloader.dataset)\n",
    "    train_accuracy = correct / count\n",
    "    \n",
    "    scheduler.step()\n",
    "    train_f1 = f1_score(np.array(full_true),np.array(full_pred),average = \"binary\")\n",
    "    return train_loss, train_accuracy, train_f1\n",
    "# valid and test\n",
    "def valid_and_test(dataloader):\n",
    "    model.eval()\n",
    "\n",
    "    val_loss, val_acc = 0.0, 0.0\n",
    "    count, correct = 0, 0\n",
    "    full_true = []\n",
    "    full_pred = []\n",
    "    for _, (sentences, labels) in enumerate(dataloader):\n",
    "        sentences, labels = sentences.to(DEVICE), labels.to(DEVICE)\n",
    "        \n",
    "        #forawrd\n",
    "        output = model(sentences) # invokes forward()\n",
    "        loss = criterion(output, labels)\n",
    "        \n",
    "        val_loss += loss.item()\n",
    "        correct += (output.argmax(1) == labels).float().sum().item()\n",
    "        count += len(sentences)\n",
    "        full_true.extend(labels.cpu().numpy().tolist())\n",
    "        full_pred.extend(output.argmax(1).cpu().numpy().tolist())\n",
    "        \n",
    "    val_loss *= batch_size\n",
    "    val_loss /= len(dataloader.dataset)\n",
    "    val_acc = correct / count\n",
    "    f1 = f1_score(np.array(full_true), np.array(full_pred), average=\"binary\")\n",
    "    return val_loss, val_acc, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:16<02:29, 16.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for epoch 1/10, train_loss: 0.5846, train_acc: 0.6845, val_loss: 0.4764, val_acc: 0.7838, test_loss: 0.4988, test_acc: 0.7913 (in average)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:33<02:15, 16.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for epoch 2/10, train_loss: 0.4213, train_acc: 0.8119, val_loss: 0.4147, val_acc: 0.8151, test_loss: 0.4079, test_acc: 0.8238 (in average)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:51<02:00, 17.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for epoch 3/10, train_loss: 0.3274, train_acc: 0.8655, val_loss: 0.3922, val_acc: 0.8279, test_loss: 0.3731, test_acc: 0.8374 (in average)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [01:08<01:43, 17.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for epoch 4/10, train_loss: 0.2524, train_acc: 0.9038, val_loss: 0.3899, val_acc: 0.8300, test_loss: 0.3706, test_acc: 0.8266 (in average)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [01:24<01:24, 16.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for epoch 5/10, train_loss: 0.1894, train_acc: 0.9319, val_loss: 0.4099, val_acc: 0.8303, test_loss: 0.3892, test_acc: 0.8266 (in average)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [01:41<01:07, 16.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for epoch 6/10, train_loss: 0.1349, train_acc: 0.9561, val_loss: 0.4159, val_acc: 0.8328, test_loss: 0.3794, test_acc: 0.8293 (in average)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [02:00<00:52, 17.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for epoch 7/10, train_loss: 0.1287, train_acc: 0.9575, val_loss: 0.4196, val_acc: 0.8319, test_loss: 0.3861, test_acc: 0.8320 (in average)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [02:19<00:35, 17.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for epoch 8/10, train_loss: 0.1238, train_acc: 0.9593, val_loss: 0.4245, val_acc: 0.8325, test_loss: 0.3862, test_acc: 0.8320 (in average)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [02:35<00:17, 17.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for epoch 9/10, train_loss: 0.1200, train_acc: 0.9616, val_loss: 0.4305, val_acc: 0.8307, test_loss: 0.3884, test_acc: 0.8374 (in average)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [02:51<00:00, 17.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for epoch 10/10, train_loss: 0.1138, train_acc: 0.9638, val_loss: 0.4329, val_acc: 0.8319, test_loss: 0.3937, test_acc: 0.8293 (in average)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "# \n",
    "for each in tqdm(range(1, EPOCHS + 1)):\n",
    "    tr_loss, tr_acc, tr_f1 = train(train_dataloader)\n",
    "    val_loss, val_acc, val_f1 = valid_and_test(val_dataloader)\n",
    "    test_loss, test_acc, test_f1 = valid_and_test(test_dataloader)\n",
    "    print(\n",
    "        f\"for epoch {each}/{EPOCHS}, train_loss: {tr_loss:.4f}, train_acc: {tr_acc:.4f}, val_loss: {val_loss:.4f}, val_acc: {val_acc:.4f}, test_loss: {test_loss:.4f}, test_acc: {test_acc:.4f} (in average)\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sentiment_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
